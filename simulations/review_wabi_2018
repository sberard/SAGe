This paper constructs an algorithmic framework which unifies and generalizes at least two lines of researches toward including biological constraints in genome rearrangement problems. The motivation is taken from several genome rearrangement problems, but the framework is larger, with the definition and resolution of a general problem on graph transformations. I think it is a very nice work on which future works will probably rely. It has a wide implication, it is very well written and explained. The results are technical but there is a long introduction to motivate and present them, so they perfectly make sense.

Given the time allowed for reviews I could not check the validity of all proofs but all results make sense in the framework, and relate well to previous results. So I think it it a good paper for Wabi, with both non trivial results in algorithmics on graphs, and good integration in bioinformatics.
 
Results are broad, including the comparison of all kinds of genomes, with unequal gene content, unequal telomeres...
The direct application to data is of course questionable, but many particular cases have already been applied on data and practical issues are discussed so the connection with biology is there.
In this line the discussion on the usual size of data is a bit restrictive. Of course there are many datasets with a few dozen or hundred blocks, but it seems to depend a lot on the resolution, chosen by the data designer. I wonder what is this notion of "effective blocks". It is a good question whether there are datasets with thousand blocks that make sense. Theoretically it would be sufficient to find couples of species with many genes and sufficient divergence.

Typos:
p11: "closer to 600" -> is closer?
Appendix B7: there is a "lemma ??", refered in p10 with ??
p4, there is a reference to example 25, but apparently no example 25 in the text or appendix
p4, Fribourg -> Friedberg
p2, the sum ... are minimized -> is?
p1, Knibb -> Knibbe
p1, Tayler -> Taylor


 
 
 